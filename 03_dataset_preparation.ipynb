{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a TensorFlow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow tensorflow-datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ivan/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/ivan/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/ivan/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/ivan/tools/python36/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/ivan/tools/python36/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python36/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python36/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.10.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ef98d34c8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ivan/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/ivan/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/ivan/tools/python_venv/python36/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/ivan/tools/python36/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/ivan/tools/python36/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bair_robot_pushing_small',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'coco2014',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'fashion_mnist',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imdb_reviews',\n",
       " 'lm1b',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'moving_mnist',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'quickdraw_bitmap',\n",
       " 'squad',\n",
       " 'starcraft_video',\n",
       " 'svhn_cropped',\n",
       " 'tf_flowers',\n",
       " 'wmt_translate_ende',\n",
       " 'wmt_translate_enfr']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetsImageNet():\n",
    "    imageNetBuilder = tfds.builder(\"imagenet2012\")\n",
    "    imageNetBuilder.download_and_prepare()\n",
    "    return ({\n",
    "        'train': imageNetBuilder.as_dataset(split=tfds.Split.TRAIN), \n",
    "        'valid': imageNetBuilder.as_dataset(split=tfds.Split.VALIDATION),\n",
    "        'test': imageNetBuilder.as_dataset(split=tfds.Split.TEST)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imageNet = getDatasetsImageNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectRawImgList(inputDir, outputDir):\n",
    "    result = {'pathnames': list(), 'labelStrings': list(), 'newPathnames': list()}\n",
    "    cnt = 0\n",
    "    with os.scandir(inputDir) as inp:\n",
    "        for entry in inp:\n",
    "            if not os.path.isdir(entry):\n",
    "                continue\n",
    "            print(\"Processing directory {}\".format(os.path.abspath(entry)))\n",
    "            category = os.path.basename(entry)\n",
    "            print(\"Category name: {}\".format(category))\n",
    "            for fileEntry in os.scandir(entry):\n",
    "                #print(\"File {}\".format(os.path.basename(fileEntry)))\n",
    "                #result.append((os.path.abspath(fileEntry), category))\n",
    "                result['pathnames'].append(os.path.abspath(fileEntry))\n",
    "                result['labelStrings'].append(category)\n",
    "                result['newPathnames'].append(os.path.join(outputDir, \"{:05}.png\".format(cnt)))\n",
    "                cnt += 1\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFileNameDataSet(inputDir, outputDir):\n",
    "    rawCollection = collectRawImgList(inputDir, outputDir)\n",
    "    \n",
    "    placePathnames = tf.placeholder(shape=(None,), dtype=tf.string)\n",
    "    placeLabelStrings = tf.placeholder(shape=(None,), dtype=tf.string)\n",
    "    placeNewPathnames = tf.placeholder(shape=(None,), dtype=tf.string)\n",
    "    \n",
    "    dataSetPathnames = tf.data.Dataset.from_tensor_slices(placePathnames)\n",
    "    dataSetLabelStrings = tf.data.Dataset.from_tensor_slices(placeLabelStrings)\n",
    "    dataSetNewPathnames = tf.data.Dataset.from_tensor_slices(placeNewPathnames)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((dataSetPathnames, dataSetLabelStrings, dataSetNewPathnames))\n",
    "    \n",
    "    return (dataset, rawCollection, (placePathnames, placeLabelStrings, placeNewPathnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformFiles(inputDir, outputDir, targetHeight=127, targetWidth=127):\n",
    "    \n",
    "    if os.path.exists(outputDir):\n",
    "        shutil.rmtree(outputDir)\n",
    "        \n",
    "    os.mkdir(outputDir)\n",
    "    outputDirOld = outputDir\n",
    "    outputDir = os.path.join(outputDir, 'img')\n",
    "    os.mkdir(outputDir)\n",
    "    \n",
    "    fnameDataset, rawCollection, placeholders = formFileNameDataSet(inputDir, outputDir)\n",
    "    \n",
    "    with open(os.path.join(outputDirOld, \"fileLabels.txt\"), \"w\") as labFile:\n",
    "        for fn, lab in zip(rawCollection['newPathnames'], rawCollection['labelStrings']):\n",
    "            labFile.write(\"'{}' {}\\n\".format(os.path.basename(fn), lab))\n",
    "        \n",
    "    \n",
    "    def _howToTransform(pathname, labelString, newPathname):\n",
    "        img = tf.image.resize_images(tf.image.decode_jpeg(tf.read_file(pathname)), \n",
    "                                     [targetHeight, targetWidth])\n",
    "        #writeOp = tf.write_file(newPathname, tf.image.encode_png(tf.cast(img, tf.uint8)))\n",
    "        return (pathname, labelString, newPathname, img)\n",
    "    \n",
    "    fnameDataset = fnameDataset.map(_howToTransform)\n",
    "    iterator = fnameDataset.make_initializable_iterator()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer, \n",
    "                 feed_dict={placeholders[0]: rawCollection['pathnames'],\n",
    "                           placeholders[1]: rawCollection['labelStrings'],\n",
    "                           placeholders[2]: rawCollection['newPathnames']})\n",
    "        nextElement = iterator.get_next()\n",
    "        writeOp = tf.write_file(nextElement[2], tf.image.encode_png(tf.cast(nextElement[3], tf.uint8)))\n",
    "        while True:\n",
    "            try:\n",
    "                val, wop = sess.run([nextElement, writeOp])\n",
    "                #print(val)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory /home/ivan/datasets/bilbao2019/raw/bench\n",
      "Category name: bench\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/textad\n",
      "Category name: textad\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/washbasin\n",
      "Category name: washbasin\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/manhole\n",
      "Category name: manhole\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/staircase\n",
      "Category name: staircase\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/urn\n",
      "Category name: urn\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/socket\n",
      "Category name: socket\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/ad\n",
      "Category name: ad\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/urinal\n",
      "Category name: urinal\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/gate\n",
      "Category name: gate\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/container\n",
      "Category name: container\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/switch\n",
      "Category name: switch\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/toilet\n",
      "Category name: toilet\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/logo\n",
      "Category name: logo\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/door\n",
      "Category name: door\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/tap\n",
      "Category name: tap\n",
      "Processing directory /home/ivan/datasets/bilbao2019/raw/extension\n",
      "Category name: extension\n"
     ]
    }
   ],
   "source": [
    "transformFiles(\"/home/ivan/datasets/bilbao2019/raw\",\n",
    "               \"/home/ivan/datasets/bilbao2019/processed\", \n",
    "               targetHeight=227, targetWidth=227)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have prepared the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/bench\n",
      "Category name: bench\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/textad\n",
      "Category name: textad\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/washbasin\n",
      "Category name: washbasin\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/manhole\n",
      "Category name: manhole\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/staircase\n",
      "Category name: staircase\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/urn\n",
      "Category name: urn\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/socket\n",
      "Category name: socket\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/ad\n",
      "Category name: ad\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/urinal\n",
      "Category name: urinal\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/gate\n",
      "Category name: gate\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/container\n",
      "Category name: container\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/switch\n",
      "Category name: switch\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/toilet\n",
      "Category name: toilet\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/logo\n",
      "Category name: logo\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/door\n",
      "Category name: door\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/tap\n",
      "Category name: tap\n",
      "Processing directory /home/ivan/datasets/bilbao2019/validation_raw/extension\n",
      "Category name: extension\n"
     ]
    }
   ],
   "source": [
    "transformFiles(\"/home/ivan/datasets/bilbao2019/validation_raw\",\n",
    "               \"/home/ivan/datasets/bilbao2019/validation_processed\", \n",
    "               targetHeight=227, targetWidth=227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
